{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "from hashlib import md5\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "sys.path.insert(0, '/home/jovyan/volcano-anomaly/notebook_pges')\n",
    "from data_utils import *\n",
    "from viz_utils import *\n",
    "from lstm_obj import lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Defining your job inputs\n",
    "\n",
    "The following cell is tagged with \"parameters\", which allows papermill to identify the cell containing per-run parameters\n",
    "Cell tags may be accessed using the double-gear icon in JupyterLab's left-hand gutter.\n",
    "\n",
    "All variables defined in the following cell are treated as job input parameters, and will be accessible through the `_context.json` file at runtime.\n",
    "\n",
    "For more information, visit https://papermill.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Job input parameters\n",
    "input_dem_err_url: str = 'http://soamc-dev-rs-fwd.s3-us-west-2.amazonaws.com/products/S1-TIMESERIES-MINTPY/v1.0/2019/02/14/S1-TIMESERIES-MINTPY-A124-20190214-718d4/demErr.h5'\n",
    "central_pixel_x: int = 500\n",
    "central_pixel_y: int = 500\n",
    "active_range_distance: int = 12\n",
    "outer_range_distance: int = 24\n",
    "lstm_grid_size: int = 3\n",
    "lstm_time_window: int = 9\n",
    "\n",
    "# PCM-System Parameters\n",
    "# These use reserved-prefix parameter names (_*) and are also parsed during `notebook-pge-wrapper specs` to generate the hysds-io and job-spec\n",
    "_time_limit = 57389\n",
    "_soft_time_limit = 4738\n",
    "_disk_usage = \"10GB\"\n",
    "_submission_type = \"individual\"\n",
    "_required_queue = \"factotum-job_worker-small\"\n",
    "_label = \"Volcano Anomaly LSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_grid_size = int(lstm_grid_size)\n",
    "lstm_time_window = int(lstm_time_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_pixel_range(central_pixel: int, distance: int):\n",
    "    return (central_pixel - distance, central_pixel + distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining your process\n",
    "\n",
    "The following cell contains trivial stubbed function examples as might be used in a job execution flow.\n",
    "\n",
    "Generally, a job consists of retrieving some data based on the job's arguments, processing it somehow, and writing the output to one or more files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_data(source_file_url: str) -> str:\n",
    "    \"\"\"Returns the filename \"\"\"\n",
    "\n",
    "    destination_filepath = os.path.abspath('./timeseries_demErr.h5')\n",
    "    \n",
    "    !wget -O {destination_filepath} {source_file_url}\n",
    "    \n",
    "    return destination_filepath\n",
    "\n",
    "def extract(dem_err_filepath, active_range, outer_range, output_dir) -> None:\n",
    "    print('EXTRACTING TRAINING DATA')\n",
    "    \n",
    "    data, dates = read_data(dem_err_filepath)\n",
    "    training_data = {}\n",
    "\n",
    "    data_time_steps = data.shape[0] - 1\n",
    "    data_x_resolution, data_y_resolution = data.shape[1:]\n",
    "\n",
    "    training_data['area'] = dict()\n",
    "    training_data['area']['dates'] = dates\n",
    "    extr_series = extract_training_data(data, active_range, outer_range)\n",
    "\n",
    "    extr_reshp = []\n",
    "    for j in range(extr_series.shape[1]):\n",
    "        for i in range(extr_series.shape[2]):\n",
    "            serie = extr_series[:, j, i]\n",
    "            assert (len(dates) == serie.shape[0])\n",
    "            extr_reshp.append(serie)\n",
    "    extr_reshp = np.array(extr_reshp)\n",
    "\n",
    "    plot_data = {'series' : extr_reshp,\n",
    "                 'dates' : dates,\n",
    "                 'label' : None,\n",
    "                 'id' : \"sierra negra\"}\n",
    "\n",
    "    # Reshape to 3D format expected by LSTM: [samples, timesteps, features]\n",
    "    extr_reshp = extr_reshp.reshape(len(dates), len(extr_reshp))\n",
    "    training_data['area']['series'] = np.expand_dims(extr_reshp, axis=0)\n",
    "\n",
    "    output_filepath = os.path.join(output_dir, \"training_data.png\")\n",
    "    plot_1dseries(plot_data, dates, os.path.abspath(output_filepath))\n",
    "\n",
    "    pickle.dump(training_data, open('training_data.p', 'wb'))\n",
    "    \n",
    "\n",
    "def train_lstm_model(training_data_filepath: str = os.path.abspath('./training_data.p')) -> None:\n",
    "    print('TRAINING_LSTM_MODEL')\n",
    "    \n",
    "    # Load training data\n",
    "    import pickle\n",
    "    data = pickle.load(open(training_data_filepath, 'rb'))['area']\n",
    "\n",
    "    # Build dataset\n",
    "    dataset = build_dataset({'series' : data['series'], 'dates' : data['dates']})\n",
    "    trainset, validset = dataset['train'], dataset['test']\n",
    "\n",
    "    ## Train and validate \n",
    "    lstm_obj = lstm(n_in=trainset['diff'].shape[1]-1, \n",
    "                    n_out=1, \n",
    "                    n_feat=trainset['diff'].shape[2],\n",
    "                    n_repeat=2)\n",
    "    lstm_obj.train_and_validate_lstm(trainset['diff'], trainset['raw'],\n",
    "                                     validset['diff'], validset['raw'])\n",
    "\n",
    "    print (f\"Selected Model Rmse Test Set: {lstm_obj.min_rmse}\")\n",
    "    return lstm_obj\n",
    "\n",
    "def infer_from_lstm_model(dem_err_filepath, lstm_obj, active_range):\n",
    "    print('INFERRING_FROM_LSTM_MODEL')\n",
    "\n",
    "    data, dates = read_data(dem_err_filepath)\n",
    "    \n",
    "    # input to lstm is \"diffed\"\n",
    "    data_active_raw = crop_volcanodata(data, active_range)\n",
    "    data_active_diff = np.diff(data_active_raw, axis=0) \n",
    "    data_active_raw = data_active_raw[1:, :, :]\n",
    "    assert (data_active_raw.shape == data_active_diff.shape)\n",
    "\n",
    "    anomaly_map = build_anomaly_map({'raw' : data_active_raw, 'diff' : data_active_diff}, lstm_obj)\n",
    "    print (anomaly_map.shape)\n",
    "    return anomaly_map\n",
    "\n",
    "def plot_anomalies(dem_err_filepath, anomaly_map, time_window, output_dir):\n",
    "    print('PRODUCING_ANOMALY_VIZUALISATIONS')\n",
    "    data, dates = read_data(dem_err_filepath)\n",
    "    plot_timeseries2d(anomaly_map, dates[time_window+1:], 'Anomaly Maps', os.path.abspath(output_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Defining your job outputs and metadata files\n",
    "\n",
    "The following cell contains the functions necessary to create a trivial data product for ingestion into the PCM data product catalog.\n",
    "\n",
    "These functions should be augmented to include your desired dataset definition data, metadata and job output files\n",
    "\n",
    "It is also typical to include important fields (e.g. track number, orbit direction and temporal bound timestamps) in the dataset id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = os.path.abspath(os.curdir)\n",
    "\n",
    "def generate_dummy_context_file() -> None:\n",
    "    \"\"\"When run in HySDS, a _context.json file will be present in the working directory, so this is only necessary for local development\"\"\"\n",
    "    filepath: str = os.path.join(working_dir, '_context.json')\n",
    "    print(f'Writing dummy context to {filepath}')\n",
    "    with open(filepath, 'w+') as context_file:\n",
    "        json.dump({'run_timestamp': datetime.now().isoformat()}, context_file)\n",
    "\n",
    "def generate_dataset_id(id_prefix: str, context: str) -> str:\n",
    "    \"\"\"Generates a globally-unique ID for the data product produced.\n",
    "    Uniqueness is generally ensured by the context, which will (theoretically) be either unique, or subject to deduplication by HySDS\"\"\"\n",
    "    \n",
    "    hash_suffix = md5(context.encode()).hexdigest()[0:5]\n",
    "\n",
    "    job_id = f'{id_prefix}-{datetime.now().isoformat()}-{hash_suffix}'\n",
    "\n",
    "    print(f'Generated job ID: {job_id}')\n",
    "    return job_id\n",
    "\n",
    "\n",
    "def generate_dataset_file(dataset_id: str, **kwargs) -> None:\n",
    "    \"\"\"Stores standardized metadata used for indexing products in HySDS GRQ\"\"\"\n",
    "    dataset_definition_filepath: str = os.path.join(working_dir, dataset_id, f'{dataset_id}.dataset.json')\n",
    "    metadata: dict = {\n",
    "        'version': kwargs.get('version', 'v1.0'),\n",
    "    }\n",
    "    \n",
    "    optional_fields = [\n",
    "        'label',\n",
    "        'location',  # Must adhere to geoJSON \"geometry\" format\n",
    "        'starttime',\n",
    "        'endtime'\n",
    "    ]\n",
    "    for field in optional_fields:\n",
    "        if field in kwargs:\n",
    "            metadata[field] = kwargs.get(field)\n",
    "    \n",
    "    with open(dataset_definition_filepath, 'w+') as dataset_file:\n",
    "        print(f'Writing to {dataset_definition_filepath}')\n",
    "        json.dump(metadata, dataset_file)\n",
    "    \n",
    "def generate_metadata_file(dataset_id: str, metadata: Dict) -> None:\n",
    "    \"\"\"Stores custom metadata keys/values used for indexing products in HySDS GRQ\"\"\"\n",
    "    metadata_filepath: str = os.path.join(working_dir, dataset_id, f'{dataset_id}.met.json')\n",
    "    with open(metadata_filepath, 'w+') as metadata_file:\n",
    "        print(f'Writing to {metadata_filepath}')\n",
    "        json.dump(metadata, metadata_file)\n",
    "        \n",
    "\n",
    "        \n",
    "def generate_data_product(working_dir: str = working_dir, id_prefix: str = 'ON_DEMAND-MY_JOB_TYPE') -> None:\n",
    "    \"\"\"Generates metadata/dataset files and packages them in a specially-named directory with the desired job output files, for ingestion into the data product catalog\"\"\"\n",
    "    from glob import glob\n",
    "    \n",
    "    context_filepath: str = os.path.join(working_dir, '_context.json') \n",
    "    with open(context_filepath) as context_file:\n",
    "        context: str = context_file.read()\n",
    "            \n",
    "    dataset_id: str = generate_dataset_id(id_prefix, context)\n",
    "    \n",
    "    data_product_dir = os.path.join(working_dir, dataset_id)\n",
    "    print(f'Generating data product at {data_product_dir}')\n",
    "    \n",
    "    os.mkdir(data_product_dir)\n",
    "    generate_metadata_file(dataset_id, {'my_metadata_field': 'metadata_value'})\n",
    "    generate_dataset_file(dataset_id)\n",
    "    \n",
    "    print(f'Moving PGE output...')\n",
    "    shutil.move(os.path.join(working_dir, 'training_data.p'), os.path.join(data_product_dir, 'training_data.p'))\n",
    "    for filepath in glob(os.path.join(working_dir, '*.png')):\n",
    "        print(filepath)\n",
    "        filename = os.path.split(filepath)[-1]\n",
    "        print(filename)\n",
    "        shutil.move(filepath, os.path.join(data_product_dir, filename))\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Defining your job's high-level execution flow\n",
    "\n",
    "The following cell contains a trivial set of procedural calls, which will be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generate_dummy_context_file()\n",
    "\n",
    "dem_err_filepath =  retrieve_data('http://soamc-dev-rs-fwd.s3-us-west-2.amazonaws.com/products/S1-TIMESERIES-MINTPY/v1.0/2019/02/14/S1-TIMESERIES-MINTPY-A124-20190214-718d4/demErr.h5')\n",
    "# dem_err_filepath = '/home/jovyan/volcano_anomaly/notebook_pges/timeseries_demErr.h5'\n",
    "print(dem_err_filepath)\n",
    "\n",
    "active_range = get_pixel_range(central_pixel_x, active_range_distance), get_pixel_range(central_pixel_y, active_range_distance) # Coordinates of volcano active region\n",
    "outer_range  = get_pixel_range(central_pixel_x, outer_range_distance), get_pixel_range(central_pixel_y, outer_range_distance) # Width and Height of this region need to be multiple of the grid size used in function \"extract_training_data\" (grid_size=3 by default)\n",
    "assert ((active_range[0][1] - active_range[0][0]) % lstm_grid_size == 0)\n",
    "assert ((active_range[1][1] - active_range[1][0]) % lstm_grid_size == 0)\n",
    "\n",
    "output_dir = os.getcwd()\n",
    "print(f'Saving plotted output to {output_dir}/')\n",
    "\n",
    "extract(dem_err_filepath, active_range, outer_range, output_dir)\n",
    "lstm_model = train_lstm_model()\n",
    "anomaly_map = infer_from_lstm_model(dem_err_filepath, lstm_model, active_range)\n",
    "plot_anomalies(dem_err_filepath, anomaly_map, lstm_time_window, output_dir)\n",
    "\n",
    "generate_data_product()\n",
    "\n",
    "print('PGE execution complete!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
